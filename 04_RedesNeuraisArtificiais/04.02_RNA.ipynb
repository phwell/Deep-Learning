{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Neurônios\n",
    "\n",
    "\n",
    "![neurônio](img/intro-to-neural-networks-scikit-learn-1.png)\n",
    "\n",
    "Como grande parte dos algoritmos de aprendizado de máquina, os neurônios são modelos matemáticos (ou funções) que representam a realidade de forma simplificada. Eles são compostos por uma soma ponderada, seguida ou não de uma função ativação. Por exemplo, considere a tarefa de prever se o preço de uma casa será maior ou menor do que a média, dadas as variáveis \\\\(x_1\\\\), o tamanho da casa em metros quadrados, \\\\(x_2\\\\), o índice de pobreza vizinhança e \\\\(x_3\\\\) o tamanho do meu cabelo. Podemos facilmente utilizar um neurônio para resolver essa tarefa. Note que, provavelmente, quanto maior \\\\(x_1\\\\), maior a probabilidade da casa ter um preço acima da média (e vice versa). Assim, devemos esperar que o peso de \\\\(x_1\\\\), \\\\(w_1\\\\), na soma ponderada do nosso neurônio seja positivo, indicando que essa variável tem um impacto igualmente positivo na probabilidade do preço da casa ser acima da média. Com o mesmo raciocínio, podemos argumentar que \\\\(w_2\\\\) será negativo. Note que esses dois pesos não precisam ter a mesma intensidade. Pode ser que o impacto positivo de \\\\(x_1\\\\) seja muito maior que o impacto negativo de \\\\(x_2\\\\), de forma que \\\\(w_1\\\\) seja maior que \\\\(w_2\\\\). Em palavras, pode ser que o tamanho da casa seja um determinante mais importante do preço do que o índice de pobreza da vizinhança. Por fim, é provável que o tamanho do meu cabelo, \\\\(x_3\\\\), não tenha muito impacto no preço de uma casa. Por isso, esperamos que \\\\(w_3\\\\) seja muito próximo de zero na soma ponderada do nosso neurônio. Isso indica que essa variável influencia pouco o preço da casa. Repare também que temos uma variável que é sempre \\\\(1\\\\). A ponderação desse \\\\(1\\\\) com o \\\\(w_0\\\\) é o que chamamos de viés. Esse viés captura a tendência da casa ter valor alto, uma vez que já tenhamos considerado as outras variáveis. Por fim, é importante ressaltar que os \\\\(w\\\\)s são o que chamamos de parâmetros do modelo. Eles são variáveis que o neurônio (e, mais para frente, a rede neural) vai aprender (ou estimar) durante o treinamento.\n",
    "\n",
    "Além da soma ponderada, nosso neurônio precisa de uma função de ativação. Isso porque a soma ponderada pode nos dar um resultado qualquer, mas, como nossa previsão é uma probabilidade, precisamos de uma função ativação que converta um número qualquer, positivo ou negativo, em um valor entre 0 e 1. A função que faz isso chama função [softmax](https://en.wikipedia.org/wiki/Softmax_function), então a usaremos após a soma ponderada do nosso neurônio. Existem várias funções de ativação e, dependendo da tarefa em questão, uma é mais recomendada do que outra. Infelizmente, para falar delas é preciso mais conhecimento matemático. Intuitivamente, quando falarmos delas nas redes neurais, pense na função ativação como algo que dá um comportamento mais complexo aos neurônios. Elas também são fundamentais nas redes neurais, para que essas consigam representar padrões complexos. \n",
    "\n",
    "## Redes Neurais Artificiais\n",
    "\n",
    "Infelizmente, os neurônios são bastante limitados. Em aprendizado de máquina, queremos que um algoritmo possa aprender qualquer tipo de padrão presente nos dados, mas isso não é possível com um simples neurônio. Por isso, construímos as redes neurais, que são simplesmente vários neurônios conectados. Pense nos neurônios como blocos de Lego e nas redes neurais como estruturas que montamos empilhando esses blocos de Lego. Dependendo da tarefa, uma estrutura pode se mais útil do que outra. No entanto, aqui, vamos considerar apenas a estrutura mais simples e mais comun de rede neural, o modelo de **redes neurais *feedforward* densas**.\n",
    "\n",
    "![neurônio](img/neural-network.png)\n",
    "\n",
    "Na rede neural acima, como exemplo, podemos dizer que ainda estamos lidando com o problema de prever se o preço de uma casa será acima ou abaixo da média. Na entrada da rede, temos as mesmas 3 variáveis mais o viés, que são representados pelas bolinhas verdes. Isso é o que chamamos de camada de entrada da rede neural. Em seguida, utilizando 5 neurônios, realizamos 5 somas ponderadas seguidas de uma função de ativação. Essas operações são representadas pelas bolinhas azuis, que recebem o nome de camada oculta da rede neural. Por fim, utilizamos um único neurônio que realiza uma soma ponderada do resultado dos neurônios anteriores e então converte essa soma ponderada em uma probabilidade com a função softmax. Isso é o que chamamos de camada de saída da rede neural e está representado pela bolinha vermelha.\n",
    "\n",
    "Ignore a camada de entrada (verde) por um momento. Note como a camada de saída mais a camada oculta é exatamente o modelo de neurônio que vimos antes? A camada de saída é simplesmente um modelo de neurônio, que está tratando a camada oculta como se fosse as variáveis independentes que determinam a variável de resposta (no nosso exemplo, a probabilidade do preço da casa ser alto). Assim, podemos ver que a rede neural está **aprendendo novas variáveis** e usando um modelo de neurônio nessas novas variáveis. Esse é o princípio básico de *deep learning*: aprender variáveis representativas, geralmente mais abstratas, que auxiliem na tarefa em questão, no caso, uma tarefa de previsão.\n",
    "\n",
    "Podemos ir ainda um passo além e adicionar uma segunda camada oculta. \n",
    "\n",
    "![neurônio](img/neural_net2.jpeg)\n",
    "\n",
    "Isso aumenta ainda mais o poder representativo da rede neural. Lembre-se de que **podemos pensar nas camadas da rede neural como níveis hierárquicos de abstração**.\n",
    "\n",
    "## Treinando RNAs\n",
    "\n",
    "Agora que entendemos o que são redes neurais em um nível intuitivo, precisamos saber como treiná-las. Isso é feito por um processo de otimização no qual minimizamos uma função custo (ou objetivo). Para manter o nível de simplicidade, pense na função custo como algo que mede a diferença entre o que a rede neural prevê o que de fato foi observado. Por exemplo, se a rede neural prever um valor pequeno para a probabilidade de uma casa ter preço acima da média, mas a casa, na verdade, for bastante cara, então a função custo terá um valor alto.\n",
    "\n",
    "Para iniciar o treinamento, vamos chutar alguns valores para os \\\\(w\\\\)s de cada neurônio. Em seguida, vamos ver a previsão da rede neural em alguns dados, que, muito provavelmente, será péssima. Dessa forma, os \\\\(w\\\\)s iniciais serão associados a um alto custo ou a uma **região elevada na superfície de custo**. No treinamento então, vamos atualizar os \\\\(w\\\\)s de maneira iterativa, de forma a diminuir o custo. Isso é feito com a técnica de gradiente descendente estocástico, que pode ser entendida como uma descida na superfície de custo de uma tarefa de otimização.\n",
    "\n",
    "![neurônio](img/perceptron_gradient_descent_1.png)\n",
    "\n",
    "Para entender a fundo essa técnica, é preciso saber cálculo multivariado, mas, intuitivamente, ela é bem simples. Em primeiro lugar, pegamos aleatoriamente (daí a palavra estocástico) um pequeno punhado de dados para conseguir uma estimativa da nossa posição na superfície de custo. Então, movemos os parâmetros \\\\(w\\\\) na direção oposta da inclinação dessa superfície. Isso é como dar um passo para baixo na superfície de custo. Com passos suficientes, nossa esperança é que os \\\\(w\\\\) nos coloque em uma região de custo (ou erro) baixa o suficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo e Treinando uma Rede Neural Artificial\n",
    "\n",
    "### Exemplo com dataset Iris\n",
    "\n",
    "Agora sabemos o que são redes neurais e quais são as diferentes etapas que precisamos executar para construir uma rede neural simples e densamente conectada. Nesta seção, tentaremos construir uma rede neural simples que preveja a classe à qual uma determinada planta de íris pertence. Usaremos a biblioteca Scikit-Learn do Python para criar nossa rede neural que executa essa tarefa de classificação. As instruções de download e instalação da biblioteca Scikit-Learn estão disponíveis em: http://scikit-learn.org/stable/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados\n",
    "O conjunto de dados que vamos usar para este tutorial é o popular conjunto de dados Iris, disponível em https://archive.ics.uci.edu/ml/datasets/iris. Os detalhes do conjunto de dados estão disponíveis no link mencionado acima.\n",
    "\n",
    "Vamos pular direto para o código. O primeiro passo é importar esse conjunto de dados para o nosso programa. Para fazer isso, usaremos a biblioteca de pandas do Python.\n",
    "\n",
    "Execute o seguinte comando para carregar o conjunto de dados da íris em um quadro de dados Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# obtendo dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "\n",
    "# Atribuir nomes de coluna ao conjunto de dados\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']\n",
    "\n",
    "# convertendo dataset para Dataframe em pandas\n",
    "irisdata = pd.read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O script acima simplesmente baixa os dados da íris, atribui os nomes, como 'comprimento da sépala', 'largura da sépala', 'comprimento da pétala', 'largura da pétala' e 'Classe' às colunas do conjunto de dados e, em seguida, carrega-o no dataframe irisdata.\n",
    "\n",
    "Para ver como esse conjunto de dados realmente se parece, execute o seguinte comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal-length</th>\n",
       "      <th>sepal-width</th>\n",
       "      <th>petal-length</th>\n",
       "      <th>petal-width</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal-length  sepal-width  petal-length  petal-width        Class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisdata.head()\n",
    "#A execução do script acima exibirá as cinco primeiras linhas do nosso conjunto de dados, conforme mostrado abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processando\n",
    "Você pode ver que nosso conjunto de dados possui cinco colunas. A tarefa é prever a classe (que são os valores da quinta coluna) à qual a planta da íris pertence, com base no comprimento da sépala, largura da sépala, comprimento da pétala e largura da pétala (as quatro primeiras colunas) . O próximo passo é dividir nosso conjunto de dados em atributos e rótulos. Execute o seguinte script para fazer isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atribuir dados das quatro primeiras colunas à variável X\n",
    "X = irisdata.iloc[:, 0:4]\n",
    "\n",
    "# Atribuir dados da primeira quinta coluna à variável y\n",
    "y = irisdata.select_dtypes(include=[object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Class\n",
       "0  Iris-setosa\n",
       "1  Iris-setosa\n",
       "2  Iris-setosa\n",
       "3  Iris-setosa\n",
       "4  Iris-setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Para ver como é y, execute o seguinte código:\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode ver que os valores na série y são categóricos. No entanto, redes neurais funcionam melhor com dados numéricos. Nossa próxima tarefa é converter esses valores categóricos em valores numéricos. Mas primeiro vamos ver quantos valores únicos temos em nossa série y. Execute o seguinte script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.Class.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos três classes únicas 'Iris-setosa', 'Iris-versicolor' e 'Iris-virginica'. Vamos converter esses valores categóricos em valores numéricos. Para fazer isso, usaremos a classe LabelEncoder do Scikit-Learn.\n",
    "\n",
    "Execute o seguinte script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "y = y.apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, se você verificar novamente valores exclusivos na série y, verá os seguintes resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.Class.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode ver que os valores categóricos foram codificados para valores numéricos, ou seja, 0, 1 e 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split\n",
    "\n",
    "Para evitar ajustes excessivos, dividiremos nosso conjunto de dados em treinamentos e divisões de teste. Os dados de treinamento serão usados para treinar a rede neural e os dados de teste serão usados para avaliar o desempenho da rede neural. Isso ajuda no problema de excesso de ajuste, porque estamos avaliando nossa rede neural em dados que eles não viram (ou seja, foram treinados) antes.\n",
    "\n",
    "Para criar divisões de treinamento e teste, execute o seguinte script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O script acima divide 80% do conjunto de dados em nosso conjunto de treinamento e os outros 20% para testar dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escala de recursos\n",
    "\n",
    "Antes de fazer previsões reais, é sempre uma boa prática dimensionar os recursos para que todos possam ser avaliados de maneira uniforme. A escala do recurso é realizada apenas nos dados de treinamento e não nos dados de teste. Isso ocorre porque, no mundo real, os dados não são dimensionados e o objetivo final da rede neural é fazer previsões sobre dados do mundo real. Portanto, tentamos manter nossos dados de teste o mais reais possível.\n",
    "\n",
    "O script a seguir executa o dimensionamento de recursos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento e Previsões\n",
    "\n",
    "### Definindo os hiper-parâmetros\n",
    "Tudo parece OK com os nossos dados. Podemos então começar a construção da rede neural. O primeiro passo é definir os hiper-parâmetros do modelo. Diferentemente dos parâmetros da rede, os \\\\(w\\\\), os hiper-parâmetros não são naturalmente aprendidos durante o treinamento e devem ser ajustados à mão. Alguns dos hiper-parâmetros mais importantes da rede neural são o número de camadas e o número de neurônios em cada camada. Esses hiper-parâmetros definem a capacidade da rede neural e, por meio deles, podemos ajustar o *trade-off* entre erro por viés e por variância. Quanto maior o número de neurônios, mais potente será a rede neural, mas maior será a probabilidade dela sofrer com sobre-ajustamento.\n",
    "\n",
    "Outros hiper-parâmetros da rede neural são o tamanho do punhado de dados usado durante a otimização e o tamanho do passo dado a cada iteração de treino. Em outras palavras, o tamanho do punhado de dados define quão precisa será nossa estimativa local da superfície de custo, enquanto que a taxa de aprendizado definirá o tamanho do passo em cada descida nessa superfície de custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo constantes\n",
    "lr = 0.1 # taxa de aprendizado\n",
    "n_iter = 10 # número de iterações de treino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E agora chegou a hora de fazer o que você estava esperando, treinar uma rede neural que possa realmente fazer previsões. Para fazer isso, execute o seguinte script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=10, learning_rate_init=0.1, max_iter=10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10), max_iter=n_iter, learning_rate_init=lr)\n",
    "mlp.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sim, com o Scikit-Learn, você pode criar uma rede neural com essas três linhas de código, que tratam da maior parte do trabalho da perna para você. Vamos ver o que está acontecendo no script acima. A primeira etapa é importar a classe MLPClassifier da biblioteca sklearn.neural_network. Na segunda linha, essa classe é inicializada com dois parâmetros.\n",
    "\n",
    "O primeiro parâmetro, hidden_layer_sizes, é usado para definir o tamanho das camadas ocultas. Em nosso script, criaremos três camadas de 10 nós cada. Não existe uma fórmula padrão para escolher o número de camadas e nós para uma rede neural e isso varia bastante dependendo do problema em questão. A melhor maneira é tentar combinações diferentes e ver o que funciona melhor.\n",
    "\n",
    "O segundo parâmetro para MLPClassifier especifica o número de iterações ou épocas que você deseja que sua rede neural execute. Lembre-se, uma época é uma combinação de um ciclo de fase de avanço e propagação traseira.\n",
    "\n",
    "Por padrão, a função de ativação 'relu' é usada com o otimizador de custos 'adam'. No entanto, você pode alterar essas funções usando os parâmetros de ativação e resolução, respectivamente.\n",
    "\n",
    "Na terceira linha, a função de ajuste é usada para treinar o algoritmo em nossos dados de treinamento, ou seja, X_train e y_train.\n",
    "\n",
    "A etapa final é fazer previsões em nossos dados de teste. Para fazer isso, execute o seguinte script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliando o algoritmo\n",
    "Criamos nosso algoritmo e fizemos algumas previsões no conjunto de dados de teste. Agora é a hora de avaliar o desempenho do nosso algoritmo. Para avaliar um algoritmo, as métricas mais usadas são uma matriz de confusão, precisão, recuperação e pontuação f1. Os métodos confusion_matrix e classification_report da biblioteca sklearn.metrics podem nos ajudar a encontrar essas pontuações. O script a seguir gera um relatório de avaliação para o nosso algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  1  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94         9\n",
      "           1       0.91      1.00      0.95        10\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.96      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "# Este código acima gera o seguinte resultado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode ver pela matriz de confusão que nossa rede neural classificou incorretamente apenas uma planta das 30 plantas em que testamos a rede. Além disso, a pontuação f1 de 0,97 é muito boa, considerando que tínhamos apenas 150 instâncias para treinar.\n",
    "\n",
    "Seus resultados podem ser ligeiramente diferentes destes, porque train_test_split divide aleatoriamente dados em conjuntos de treinamento e teste, portanto, nossas redes podem não ter sido treinadas / testadas nos mesmos dados. Mas, no geral, a precisão também deve ser superior a 90% em seus conjuntos de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from IPython import display\n",
    "\n",
    "# Implementação do perceptron\n",
    "class perceptron(object):\n",
    "\n",
    "    # tol é um limiar para parar \n",
    "    def __init__(self, lrate = 1, w=None, maxiter=3, tol = 0, plot_= False):\n",
    "        self.lrate = lrate\n",
    "        self.maxiter = maxiter\n",
    "        self.tol = tol\n",
    "        self.plot_ = plot_\n",
    "        self.w = w # peso\n",
    "\n",
    "    # atualiza pesos\n",
    "    def __update_w(self, X_train, y_train):\n",
    "        for xi, yi in zip(X_train, y_train):\n",
    "            if int(np.sign(np.dot(xi, self.w.T))) != int(yi):\n",
    "                self.w += (xi.T * self.lrate)*yi\n",
    "\n",
    "    def __eval_perceptron(self, X_train, y_train):\n",
    "        error = 0\n",
    "        miss_indx = []\n",
    "        for xi, yi, i in zip(X_train, y_train, range(len(y_train))):\n",
    "            # multiplica entrada com pesos\n",
    "            # verifica se coincide com o esperado então computa erros\n",
    "            if int(np.sign(np.dot(xi, self.w.T))) != int(yi):\n",
    "                error += 1\n",
    "        return error\n",
    "\n",
    "    def _plot(self, X, y, true_w=None):\n",
    "        a, b = -self.w[1]/self.w[2], -self.w[0]/self.w[2] \n",
    "        l = np.linspace(-1,1)\n",
    "        plt.plot(l, a*l+b, 'green')\n",
    "        cols = {1: 'r', -1: 'b'}\n",
    "\n",
    "        for x,s in zip(X, y):\n",
    "            plt.plot(x[0], x[1], cols[s]+'o')\n",
    "\n",
    "        if not true_w is None:\n",
    "            a, b = -true_w[1]/true_w[2], -true_w[0]/true_w[2] \n",
    "            plt.plot(l, a*l+b, '-k')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        plt.clf() # limpa a imagem do gráfico\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = np.array(X_train)\n",
    "        X_train = np.insert(X_train, 0, 1, 1) # adiciona vies\n",
    "        y_train = np.array(y_train)\n",
    "\n",
    "        if self.w is None:\n",
    "            # se não tiver peso será definido\n",
    "            self.w = np.random.normal(0, 10, len(X_train[0]))\n",
    "        count = 0\n",
    "        \n",
    "        # loop  infinito\n",
    "        while True: \n",
    "            error = self.__eval_perceptron(X_train, y_train)\n",
    "            self.__update_w(X_train, y_train) # atualiza peso\n",
    "\n",
    "            count += 1\n",
    "            if error < self.tol or count > self.maxiter:\n",
    "                break\n",
    "\n",
    "            if self.plot_:\n",
    "                self._plot(X_train[:, 1:], y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = np.insert(X_test, 0, 1, 1)\n",
    "        return np.sign(np.dot(X_test, self.w.T))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # gera dados linearmente separáveis em 2D e os dados são randômicos\n",
    "    x1,y1,x2,y2 = [np.random.uniform(-1, 1) for i in range(4)] # define 2 pontos\n",
    "    w_target = np.array([x2*y1-x1*y2, y2-y1, x1-x2]) # gera vetor\n",
    "    a, b = -w_target[1]/w_target[2], -w_target[0]/w_target[2] # para desenhar\n",
    "\n",
    "    X = np.random.uniform(-1, 1, (100, 2)) # gera 100 pontos \n",
    "    y = np.sign(np.dot(np.insert(X, 0, 1, 1), w_target.T)) # gera targets\n",
    "\n",
    "    # usa perceptron para separar os dados\n",
    "    clf = perceptron(maxiter=10, lrate = 0.1, plot_ = True)\n",
    "    clf.fit(X, y)\n",
    "    pred = clf.predict(X)\n",
    "    print(sum(pred != y)/len(y)) # proporção de acertos\n",
    "\n",
    "    # mostra resultados\n",
    "    clf._plot(X, y, true_w = w_target)\n",
    "    \n",
    "%matplotlib notebook    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([527.33057378])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(500, 550, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  5  0]\n",
      " [ 0 13  0]\n",
      " [ 0 12  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.43      1.00      0.60        13\n",
      "           2       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.43        30\n",
      "   macro avg       0.14      0.33      0.20        30\n",
      "weighted avg       0.19      0.43      0.26        30\n",
      "\n",
      "[[ 0  5  0]\n",
      " [ 0 13  0]\n",
      " [ 0 12  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.43      1.00      0.60        13\n",
      "           2       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.43        30\n",
      "   macro avg       0.14      0.33      0.20        30\n",
      "weighted avg       0.19      0.43      0.26        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings as warn\n",
    "from sklearn.linear_model import Perceptron\n",
    "warn.filterwarnings('ignore')\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "XIris = iris.data\n",
    "yIris = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(XIris, yIris, test_size = 0.2)\n",
    "\n",
    "\n",
    "clf = perceptron(maxiter=1000, lrate = 0.01, plot_ = False)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "# sklearn\n",
    "\n",
    "# tol é o critério de parada. Se não for nenhum, as iterações irão parar\n",
    "#  quando (perda> perda anterior - tol).\n",
    "# max_iter -épocas, ou seja, o número máximo iterações sobre os dados de treinamento\n",
    "clfsklearn = Perceptron(tol=1e-3, random_state=0)\n",
    "clfsklearn.fit(X_train, y_train)\n",
    "predsklearn = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test,predsklearn))\n",
    "print(classification_report(y_test,predsklearn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 0 15  3]\n",
      " [ 0  0 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       1.00      0.83      0.91        18\n",
      "           3       0.79      1.00      0.88        11\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.94      0.93        45\n",
      "weighted avg       0.95      0.93      0.93        45\n",
      "\n",
      "Acurácia 0.933 \n",
      "[[16  0  0]\n",
      " [ 0 16  2]\n",
      " [ 0  0 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       1.00      0.89      0.94        18\n",
      "           3       0.85      1.00      0.92        11\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.95      0.96      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n",
      "Acurácia 1.000 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import shuffle\n",
    "import warnings as warn\n",
    "from sklearn import datasets\n",
    "\n",
    "class MLP(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hid_nums=[100],\n",
    "                 epochs=1000,\n",
    "                 r=0.5,\n",
    "                 batch_size=20):\n",
    "\n",
    "        self.hid_nums = hid_nums\n",
    "        self.epochs = epochs\n",
    "        self.r = r\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __ltov(self, n, label):\n",
    "        \"\"\"\n",
    "        trasform label scalar to vector\n",
    "        Args:\n",
    "        n (int) : number of class, number of out layer neuron\n",
    "        label (int) : label\n",
    "        Exmples:\n",
    "        \"\"\"\n",
    "        return [0 if i != label else 1 for i in range(1, n + 1)]\n",
    "\n",
    "    def __add_bias(self, x_vs):\n",
    "        \"\"\"\n",
    "        add bias to list\n",
    "        Args:\n",
    "        x_vs [[float]] Array: vec to add bias\n",
    "        Returns:\n",
    "        [float]: added vec\n",
    "        Examples:\n",
    "        >>> mlp = MLP()\n",
    "        >>> mlp._MLP__add_bias(np.array([[1,2,3], [1,2,3]]))\n",
    "        array([[ 1.,  2.,  3.,  1.],\n",
    "               [ 1.,  2.,  3.,  1.]])\n",
    "        \"\"\"\n",
    "        return np.c_[x_vs, np.ones(x_vs.shape[0])]\n",
    "\n",
    "    def __get_delta(self, w, delta, u):\n",
    "        return self.__dsigmoid(u) * np.dot(w.T, delta)\n",
    "\n",
    "    def __sigmoid(self, x, a=1):\n",
    "        \"\"\"\n",
    "        sigmoid function\n",
    "        Args:\n",
    "        x float\n",
    "        Returns:\n",
    "        float\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-a * x))\n",
    "\n",
    "    def __dsigmoid(self, x, a=1):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid function\n",
    "        Args:\n",
    "        x float\n",
    "        Returns:\n",
    "        float\n",
    "        \"\"\"\n",
    "        return a * self.__sigmoid(x) * (1.0 - self.__sigmoid(x))\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = self.__add_bias(x)\n",
    "        y = x.T\n",
    "\n",
    "        for w in self.Ws:\n",
    "            y = self.__sigmoid(np.dot(w, y))\n",
    "\n",
    "        if self.out_num == 1:\n",
    "            y = y.T\n",
    "            y[y < 0.5] = -1\n",
    "            y[y >= 0.5] = 1\n",
    "            return y\n",
    "        else:\n",
    "            return np.argmax(y.T, 1) + np.ones(y.T.shape[0])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # inicializar pesos\n",
    "        self.Ws = []\n",
    "\n",
    "        # número de dados de treinamento\n",
    "        self.data_num = X.shape[0]\n",
    "        assert(self.data_num > self.batch_size)\n",
    "\n",
    "        # número de neurônios de saída\n",
    "        self.out_num = max(y)\n",
    "\n",
    "        # corrigir rótulo de dados (inteiro) para array\n",
    "        y = np.array([self.__ltov(self.out_num, _y) for _y in y])\n",
    "\n",
    "        # adicionar bias\n",
    "        X = self.__add_bias(X)\n",
    "\n",
    "        # inicializar vetores de peso ocultos aleatoriamente\n",
    "        n = X.shape[1]\n",
    "        for m in self.hid_nums:\n",
    "            np.random.seed()\n",
    "            self.Ws.append(np.random.uniform(-1., 1., (m, n)))\n",
    "            n = m\n",
    "\n",
    "        # inicializar vecotors de peso de saída aleatoriamente\n",
    "        np.random.seed()\n",
    "        self.Ws.append(np.random.uniform(-1., 1., (self.out_num, n)))\n",
    "\n",
    "        # fitting\n",
    "        for i in range(self.epochs):\n",
    "            X, y = shuffle(X, y, random_state=np.random.RandomState())\n",
    "            for index in range(0, self.data_num, self.batch_size):\n",
    "\n",
    "                end = index + self.batch_size\n",
    "                end_index = end if end <= self.data_num else self.data_num\n",
    "\n",
    "                _x = X[index:end_index]\n",
    "                _y = y[index:end_index]\n",
    "\n",
    "                zs, us, deltas, dws = [], [], [], []\n",
    "\n",
    "                z = _x.T\n",
    "                zs.append(z)\n",
    "\n",
    "                for w in self.Ws:\n",
    "                    u = np.dot(w, z)\n",
    "                    z = self.__sigmoid(u)\n",
    "                    us.append(u)\n",
    "                    zs.append(z)\n",
    "\n",
    "                # adicionar bias\n",
    "                for i in range(1, len(zs) - 1):\n",
    "                    zs[i][-1] = np.full((1, zs[i].shape[1]), -1.)\n",
    "\n",
    "                delta = (z - _y.T) * self.__dsigmoid(u)\n",
    "                deltas.append(delta)\n",
    "\n",
    "                for _u, _w in zip(reversed(us[:-1]), reversed(self.Ws)):\n",
    "                    delta = self.__get_delta(_w, delta, _u)\n",
    "                    deltas.append(delta)\n",
    "\n",
    "                for _delta, _z in zip(reversed(deltas), zs[:-1]):\n",
    "                    dws.append(np.dot(_delta, _z.T) / self.batch_size)\n",
    "\n",
    "                for j, dw in enumerate(dws):\n",
    "                    self.Ws[j] -= self.r * dw\n",
    "        return self\n",
    "\n",
    "def main():\n",
    "\n",
    "    from sklearn import datasets\n",
    "    from sklearn.preprocessing import normalize\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    iris = datasets.load_iris()\n",
    "    XIris = iris.data\n",
    "    yIris = iris.target\n",
    "    mlp = MLP(hid_nums=[5], epochs=1000, r=0.1, batch_size=1)\n",
    "\n",
    "    yIris=yIris+1\n",
    "    X_train, X_test, y_train, y_test = train_test_split(XIris, yIris, test_size = 0.3)\n",
    "\n",
    "    mlp.fit(X_train, y_train)\n",
    "    pred = mlp.predict(X_test)\n",
    "    \n",
    "    print(confusion_matrix(y_test,pred))\n",
    "    print(classification_report(y_test,pred))\n",
    "    \n",
    "    print(\"Acurácia %0.3f \" % mlp.score(X_test, y_test))\n",
    "    \n",
    "    \n",
    "    #sklearn\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    mlpsklearn = MLPClassifier(hidden_layer_sizes=(10,10), max_iter=1000, learning_rate_init=lr)\n",
    "    mlpsklearn.fit(X_train, y_train)\n",
    "    predictions_mlpsklearn = mlpsklearn.predict(X_test)\n",
    "    print(confusion_matrix(y_test,predictions_mlpsklearn))\n",
    "    print(classification_report(y_test,predictions_mlpsklearn))\n",
    "    print(\"Acurácia %0.3f \" % mlpsklearn.score(X_test, predictions_mlpsklearn))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99001676 0.00667068 0.00331256]\n"
     ]
    }
   ],
   "source": [
    "# Função de ativação\n",
    "\n",
    "import numpy as np\n",
    "# transfer function\n",
    "\n",
    "def stepFunction(soma):\n",
    "    if (soma >= 1):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def sigmoidFunction(soma):\n",
    "    return 1 / (1 + np.exp(-soma))\n",
    "\n",
    "def tahnFunction(soma):\n",
    "    return (np.exp(soma) - np.exp(-soma)) / (np.exp(soma) + np.exp(-soma))\n",
    "\n",
    "def reluFunction(soma):\n",
    "    if soma >= 0:\n",
    "        return soma\n",
    "    return 0\n",
    "\n",
    "def linearFunction(soma):\n",
    "    return soma\n",
    "\n",
    "def softmaxFunction(x):\n",
    "    ex = np.exp(x)\n",
    "    return ex / ex.sum()\n",
    "\n",
    "teste = stepFunction(-1)\n",
    "teste = sigmoidFunction(-0.358)\n",
    "teste = tahnFunction(-0.358)\n",
    "teste = reluFunction(0.358)\n",
    "teste = linearFunction(-0.358)\n",
    "valores = [7.0, 2.0, 1.3]\n",
    "print(softmaxFunction(valores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoidFunction(soma):\n",
    "    return 1 / (1 + np.exp(-soma))\n",
    "teste = sigmoidFunction(0)\n",
    "print(teste)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda38167b8c1ef44120b52038489a440cfb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
