{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geração de texto com LSTM\n",
    "\n",
    "\n",
    "## Implementando geração de texto LSTM em nível de caractere\n",
    "\n",
    "\n",
    "Vamos colocar essas idéias em prática em uma implementação Keras. A primeira coisa que precisamos é de muitos dados de texto que possamos usar para aprender um\n",
    "modelo de linguagem. Você pode usar qualquer arquivo de texto suficientemente grande ou conjunto de arquivos de texto - Wikipedia, o Senhor dos Anéis, etc.\n",
    "Por exemplo, usaremos alguns dos escritos de Nietzsche, o filósofo alemão do final do século 19 (traduzidos para o inglês). O modelo de linguagem\n",
    "vamos aprender que será, portanto, especificamente um modelo do estilo de escrita de Nietzsche e tópicos de escolha, ao invés de um modelo mais genérico do\n",
    "Língua Inglesa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando os dados\n",
    "\n",
    "Vamos começar baixando o corpus e convertendo-o em letras minúsculas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "606208/600901 [==============================] - 32s 53us/step\n",
      "Corpus length: 600901\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, iremos extrair sequências parcialmente sobrepostas de comprimento `maxlen`, codificá-las one-hot e empacotá-las em uma matriz Numpy 3D `x` de\n",
    "forma `(sequências, maxlen, caracteres_unicos)`. Simultaneamente, preparamos um array `y` contendo os alvos correspondentes: o one-hot\n",
    "caracteres codificados que vêm logo após cada sequência extraída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200281\n",
      "Unique characters: 59\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Comprimento das sequências de caracteres extraídas\n",
    "maxlen = 60\n",
    "\n",
    "# Amostramos uma nova sequência a cada caractere `step`\n",
    "step = 3\n",
    "\n",
    "# Isso contém nossas sequências extraídas\n",
    "sentences = []\n",
    "\n",
    "# Isso mantém os alvos (os caracteres de acompanhamento)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Número de sequências:', len(sentences))\n",
    "\n",
    "# Lista de caracteres únicos no corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Caracters únicos:', len(chars))\n",
    "\n",
    "# Dicionário mapeando caracteres únicos para seu índice em `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Em seguida, codifique one-hot os caracteres em matrizes binárias.\n",
    "print('Vetorização...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo a rede\n",
    "\n",
    "Nossa rede é uma única camada `LSTM` seguida por um classificador` Dense` e softmax sobre todos os caracteres possíveis. Mas vamos notar que\n",
    "as redes neurais recorrentes não são a única maneira de gerar a sequência de dados; Convnets 1D também provaram ser extremamente bem sucedidos nisso em\n",
    "recentemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que nossos alvos são codificados em um ponto, usaremos `categorical_crossentropy` como a perda para treinar o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinar o modelo de linguagem e fazer sua amostragem\n",
    "\n",
    "\n",
    "Dado um modelo treinado e um snippet de texto original, geramos novo texto repetidamente:\n",
    "\n",
    "* 1) Desenhar a partir do modelo uma distribuição de probabilidade sobre o próximo caractere de acordo com o texto disponível até agora\n",
    "* 2) Reavaliando a distribuição a uma certa \"temperatura\"\n",
    "* 3) Amostragem do próximo caractere aleatoriamente de acordo com a distribuição reponderada\n",
    "* 4) Adicionar o novo caractere no final do texto disponível\n",
    "\n",
    "Este é o código que usamos para reponderar a distribuição de probabilidade original que sai do modelo,\n",
    "e extrair um índice de caracteres a partir dele (a \"função de amostragem\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, este é o loop em que treinamos e geramos texto repetidamente. Começamos a gerar texto usando uma gama de temperaturas diferentes\n",
    "depois de cada época. Isso nos permite ver como o texto gerado evolui conforme o modelo começa a convergir, bem como o impacto de\n",
    "temperatura na estratégia de amostragem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "1565/1565 [==============================] - 162s 104ms/step - loss: 1.3281\n",
      "--- Gerando com semente: \"he was annoyed by the grandiose manner, the\n",
      "mise en scene st\"\n",
      "------ temperatura: 0.2\n",
      "he was annoyed by the grandiose manner, the\n",
      "mise en scene still long and their sense of the subjugate to the spirit of the feeling of the subjugate of the subjugate of the subjugate of the spirits of the standard of the subjugate the more subjugate of the subjugate of the problem of the supposing still a self-concere the states of the sense of the subjugate the same their sense of the spirits of the soul and the highest for the same thing to the extrainone\n",
      "------ temperatura: 0.5\n",
      "he soul and the highest for the same thing to the extrainoness, the power of the new same men of their thing of his standard of the sense, the ture school-path--above them in the free spirits the sense of the preserved the life of a theological proportions of the german effect conditions of the opposite with the faith.--the distinction of him, and the time of the spirits asharatician. for\n",
      "instance, as the platabising\n",
      "moral narrow, the result, and reserded \n",
      "------ temperatura: 1.0\n",
      ", as the platabising\n",
      "moral narrow, the result, and reserded grimjfur we in .--possehtorated--whenever in so mofting! why they partic rarely will\n",
      "presenceness of felthner. comparel's pweered lead his was with the finge deceit to other cigculer the proper misunderstand the meriousne, merionly say, hesitatical that will. necessity of order their inatubular, of\n",
      "a signife may hil, with which there is\n",
      "a regarded castent\n",
      "funlt the tible, immutedness, mireult far,\n",
      "------ temperatura: 1.2\n",
      " regarded castent\n",
      "funlt the tible, immutedness, mireult far, i whol-quines resps--degree. it manner, anything is intoning: the badly their most germans and\n",
      "we knjeicke man was we evcence is supeolowed refe. finally, he hehe mala fece \"seeks himself grown out of itness ancesture, ake them lo\"s pute treitn of the\n",
      "theistread: ihulter knowledge, every cause of disverlpent finers! finds spey\n",
      "soliture.st\n",
      "be ay shapin; these e. these did or feminiy, que-conscalli\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "#for epoch in range(1, 60):\n",
    "for epoch in range(1, 2):\n",
    "    print('epoch', epoch)\n",
    "    # Ajustar o modelo para 1 época nos dados de treinamento disponíveis\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    # Selecione uma semente de texto aleatoriamente\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Gerando com semente: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperatura:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # Geramos 400 caracteres\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode ver, uma temperatura baixa resulta em texto extremamente repetitivo e previsível, mas onde a estrutura local é altamente realista: em\n",
    "Em particular, todas as palavras (sendo uma palavra um padrão local de caracteres) são palavras inglesas reais. Com temperaturas mais altas, o texto gerado\n",
    "torna-se mais interessante, surpreendente e até criativo; às vezes pode inventar palavras completamente novas que soam um tanto plausíveis (como\n",
    "\"eterned\" ou \"troveration\"). Com uma temperatura alta, a estrutura local começa a quebrar e a maioria das palavras parecem strings semi-aleatórias\n",
    "de personagens. Sem dúvida, aqui 0,5 é a temperatura mais interessante para geração de texto nesta configuração específica. Sempre experimente\n",
    "com múltiplas estratégias de amostragem! Um equilíbrio inteligente entre estrutura aprendida e aleatoriedade é o que torna a geração interessante.\n",
    "\n",
    "Observe que ao treinar um modelo maior, mais longo, em mais dados, você pode obter amostras geradas que parecerão muito mais coerentes e\n",
    "realista do que o nosso. Mas, claro, não espere gerar qualquer texto significativo, a não ser por acaso: tudo o que estamos fazendo é\n",
    "dados de amostragem de um modelo estatístico de quais personagens vêm depois de quais personagens. A linguagem é um canal de comunicação, e há\n",
    "uma distinção entre o que são as comunicações e a estrutura estatística das mensagens nas quais as comunicações são codificadas. Para\n",
    "para comprovar essa distinção, aqui está um experimento de pensamento: e se a linguagem humana fizesse um trabalho melhor em compactar as comunicações, bem como\n",
    "nossos computadores fazem com a maioria de nossas comunicações digitais? Então a linguagem não seria menos significativa, mas careceria de qualquer\n",
    "estrutura estatística, tornando impossível aprender um modelo de linguagem como acabamos de aprender.\n",
    "\n",
    "\n",
    "## Aprendizado\n",
    "\n",
    "* Podemos gerar dados de sequência discreta treinando um modelo para prever os próximos tokens dados os tokens anteriores.\n",
    "* No caso do texto, esse modelo é denominado \"modelo de linguagem\" e pode ser baseado em palavras ou caracteres.\n",
    "* Amostrar o próximo token requer equilíbrio entre seguir o que o modelo julga provável e introduzir aleatoriedade.\n",
    "* Uma maneira de lidar com isso é a noção de _softmax temperature_. Sempre experimente diferentes temperaturas para encontrar a \"certa\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
